{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "import pandas as pd\n",
    "import umap # First time you run this enter pip install umap-learn in your Anaconda Prompt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining data and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtx_to_df (mtf, columns_file, rows_file):\n",
    "    \"\"\"Takes and mtf file path, a column file path and a row file path of an mtf file and returns a pandas dataframe\"\"\"\n",
    "    import pandas as pd\n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = io.mmread(mtf)\n",
    "    with open(columns_file) as file:\n",
    "        for line in file:\n",
    "            cols.append(line.rstrip())\n",
    "    \n",
    "    with open(rows_file) as file:\n",
    "        for line in file:\n",
    "            rows.append(line.rstrip().split('\\t')[0])\n",
    "    arr = data.toarray()\n",
    "    pd.DataFrame(arr, index = rows, columns = cols).to_csv('norm_counts_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 'E-GEOD-100911.aggregated_filtered_normalised_counts.mtx'\n",
    "c = 'E-GEOD-100911.aggregated_filtered_normalised_counts.mtx_cols'\n",
    "r = 'E-GEOD-100911.aggregated_filtered_normalised_counts.mtx_rows'\n",
    "\n",
    "#mtx_to_df(norm,c,r)\n",
    "norm_counts = pd.read_csv('norm_counts_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was run to convert the gene transcript IDs to KEGG IDs, since this process took so long\n",
    "# the result was saved to a csv (geneID.csv) file and this code is not needed anymore. \n",
    "\n",
    "# from Bio import Entrez\n",
    "# import time\n",
    "# import csv\n",
    "\n",
    "# Entrez.email = \"mdpouls1@gmail.com\"\n",
    "# geneIDs = []\n",
    "# rowsTest = rows[15000:]\n",
    "# i = 0\n",
    "# while i < len(rowsTest):\n",
    "#     for t in range(3):\n",
    "#         if i < len(rowsTest):\n",
    "#             handle = Entrez.esearch(db=\"gene\", term=rowsTest[i])\n",
    "#             record = Entrez.read(handle)\n",
    "#             if len(record['IdList']) == 1:\n",
    "#                 geneIDs.append(record['IdList'][0])\n",
    "#             elif len(record['IdList']) > 1:\n",
    "#                 geneIDs.append('multiple')\n",
    "#             else:\n",
    "#                 geneIDs.append('NaN')\n",
    "#             handle.close()\n",
    "#             i += 1\n",
    "#     time.sleep(1)\n",
    "\n",
    "# with open('geneID.csv', 'a') as f:\n",
    "#     for gene in geneIDs:\n",
    "#         f.write('{},'.format(gene))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnnotations(uniprotData):\n",
    "    \"\"\"Takes the uniprotData file and creates a csv file with the gene ontologys (biological process) for each gene.\n",
    "    uniprot chart must include a gene ontology (biological process) column and either a Ids column or Gene names column. \n",
    "    The created csv has the gene ontologies as  columns and genes as rows with a 0 if the gene does not contain the GO term \n",
    "    and a 1 if it does\"\"\"\n",
    "    \n",
    "    #read the csv into a dataframe\n",
    "    annotationData = pd.read_table(uniprotData)\n",
    "    \n",
    "    #get a list of all the used GO terms in the dataset\n",
    "    GO = []\n",
    "    go_dict = {}\n",
    "    for line in annotationData['Gene ontology (biological process)'].str.split(';'):\n",
    "        if type(line) == list:\n",
    "            for item in line:\n",
    "                GO.append(item.strip())\n",
    "\n",
    "    #create a dictionary with an empty list for each GO term\n",
    "    for term in GO:\n",
    "        if term not in go_dict:\n",
    "            go_dict[term] = []\n",
    "\n",
    "    #check each gene \n",
    "    for line in annotationData['Gene ontology (biological process)'].str.split(';'):\n",
    "        item_dict = {}\n",
    "        if type(line) == list:\n",
    "            for item in line:\n",
    "                item_dict[item.strip()] = True\n",
    "            for k,v in go_dict.items():\n",
    "                if k in item_dict:\n",
    "                    v.append(1)\n",
    "                else:\n",
    "                    v.append(0)\n",
    "\n",
    "\n",
    "    go_df = pd.DataFrame.from_dict(go_dict)\n",
    "    \n",
    "    if \"Ids\" in annotationData.columns:\n",
    "        go_df['geneId'] = annotationData['Ids']\n",
    "        go_df = go_df.set_index('geneId')\n",
    "    \n",
    "    if 'Gene names' in annotationData.columns:\n",
    "        go_df['geneName'] = annotationData['Gene names']\n",
    "        go_df = go_df.drop_duplicates(subset = 'geneName')\n",
    "        go_df = go_df.set_index('geneName')\n",
    "\n",
    "    go_df.to_csv('goData.csv')\n",
    "    return(go_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCellGOTerms(norm_counts,GO_dataframe, filepathName):\n",
    "    \"\"\"Takes the normalized counts as a dataframe of normalized counts with cell IDs as the columns and gene names as the rows.\n",
    "    The normalized counts dataframe must have the column with the gene IDs named '1'. the GO_dataframe parameter is the output \n",
    "    from the getAnnotations function. The filepathName is the name of the file you want the csv to be written to\n",
    "    \n",
    "    Writes a csv of the relative amounts of a gene ontology term a cell has based off the differentially expressed genes. The\n",
    "    csv has the GO term as colums and the cell ID as rows.\"\"\"\n",
    "    \n",
    "    norm_counts_id = norm_counts.set_index(\"1\")\n",
    "    srr_dict = norm_counts_id.to_dict()\n",
    "\n",
    "\n",
    "    #Convert the go_df datafram into a dictionary of lists for the indices, columns, and data\n",
    "    gene_dict = GO_dataframe.to_dict('split')\n",
    "\n",
    "    #create a dictionary with the gene ID as the key and a list of the gene ontologies for that gene\n",
    "    geneID_dict = {}\n",
    "    for ind, data in gene_dict.items():\n",
    "        geneID_dict[ind] = data\n",
    "\n",
    "\n",
    "    ids = gene_dict['index']\n",
    "    data = gene_dict['data']\n",
    "\n",
    "    geneID_dict = {}\n",
    "    for i in range(len(ids)):\n",
    "        geneID_dict[ids[i]] = data[i]\n",
    "\n",
    "    srr_genes_dict_values = {}\n",
    "    for srr,genes in srr_dict.items():\n",
    "        genes_GO = np.zeros((len(GO_dataframe.columns),), dtype=int)\n",
    "        for gene,value in genes.items():\n",
    "            if value > 0:\n",
    "                if gene in geneID_dict:\n",
    "                    genes_GO = np.add(genes_GO,np.multiply(value, np.array(geneID_dict[gene])))\n",
    "        srr_genes_dict_values[srr] = genes_GO\n",
    "\n",
    "    #convert to a dataframe\n",
    "    cols = GO_dataframe.columns\n",
    "    gene_annotation_values_df = pd.DataFrame.from_dict(srr_genes_dict_values, orient='index', columns = cols)\n",
    "    #write data to a csv file\n",
    "    gene_annotation_values_df.to_csv('{}.csv'.format(filepathName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gagnonData_path = \"uniprot_gagnon.tab.gz\"\n",
    "# toyData_path = \"uniprotData.tab\"\n",
    "\n",
    "# testis = getAnnotations(gagnonData_path)\n",
    "# kidney = getAnnotations(toyData_path)\n",
    "\n",
    "# getCellGOTerms(norm_counts,testis,'GeneAnnotation_gagnon')\n",
    "# getCellGOTerms(norm_counts,kidney,'GeneAnnotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_counts = norm_counts.set_index('Unnamed: 0')\n",
    "norm_transpose = norm_counts.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Reduction:\n",
    "\n",
    "See the UMAP Documentation page for details. The module import cell at the top of this notebook has instructions for downloading the scikit learn plug-in for UMAP.\n",
    "\n",
    "NOTE This is a first pass - I'm not super sure it is ready to go. Next steps\n",
    "\n",
    "1 We need to cluster the data and assign tags to the data points\n",
    "\n",
    "2 Add a third dimension to the UMAP reduction\n",
    "\n",
    "3 Plot data in altair to allow interaction\n",
    "\n",
    "4 Add interaction steps, tags, whatever else we're doing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_counts_temp = norm_counts\n",
    "norm_counts_temp = norm_counts_temp.drop(['geneIDs'],axis=1).transpose()\n",
    "norm_vals = norm_counts_temp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vals_scaled = StandardScaler().fit_transform(norm_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_reducer = umap.UMAP()\n",
    "reduced_genes = UMAP_reducer.fit_transform(norm_vals_scaled)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(reduced_genes[:,0],reduced_genes[:,1])\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = norm_vals_scaled\n",
    "# PCA\n",
    "pca_mod = PCA(n_components = 7)\n",
    "data_pca = pca_mod.fit_transform(X)\n",
    "PCs =pca_mod.components_\n",
    "PCs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_mod = umap.UMAP(n_neighbors = 7, min_dist = 0.2, n_components = 3).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means\n",
    "y_pred = KMeans(n_clusters=7, max_iter=5).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors = ListedColormap(sns.color_palette('bright', 7).as_hex())\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(UMAP_mod[:, 0], UMAP_mod[:, 1],zs= UMAP_mod[:, 2], c=y_pred, cmap= colors, s=20)\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1], c = y_pred, cmap = colors, s=15)\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top gene ontologies:  \n",
    "**The** top gene ontologies are defined in this project as those that have the largest (absolute) parameter value in the final equation for the principal direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = pd.read_csv('GeneAnnotation.csv')\n",
    "annot_vals = annotation.values\n",
    "\n",
    "annot_scaled = StandardScaler().fit_transform(annot_vals)\n",
    "\n",
    "\n",
    "# PCA on gene ontologies: Use this to figure out het\n",
    "pca_mod = PCA(n_components = 1) \n",
    "data_pca = pca_mod.fit_transform(annot_scaled)\n",
    "PCs =pca_mod.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs_df = pd.DataFrame(PCs)\n",
    "PCs_df = PCs_df.transpose()\n",
    "PCs_df[1] = abs(PCs_df[0])\n",
    "topGo = PCs_df[1].sort_values(ascending = False).head(100).index #List of the index of the top gene ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_annotations = annotation.iloc[:,topGo] # Grab the top 100 annotations\n",
    "labels=top_annotations.columns\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "top_annotations[:] = min_max_scaler.fit_transform(top_annotations.values) #Scale the data to be in range [0,1]\n",
    "top_annotations.columns = np.arange(0,len(top_annotations.columns))\n",
    "top_annotations.head() #df with only the top 100 most important group ontologies listed for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization:\n",
    "**Overview**\n",
    " - The 'base' plot is the 2-D UMAP plot plotted using Altair. \n",
    " - A user can then select a region-of-interest to create the breakout histogram plots of predominate gene ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create visualization dataframe:  \n",
    "**Overview** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'base' dataframe:\n",
    "column_names = [\"UMAP x\",\"UMAP y\"]\n",
    "vis_data = pd.DataFrame(reduced_genes, columns = column_names)\n",
    "vis_data[\"Clusters\"] = y_pred # Add clusters:\n",
    "vis_data['IDs'] = top_annotations.index.values\n",
    "# vis_data[top_annotations.columns] = top_annotations.values # Bring in the 'top annotations'\n",
    "\n",
    "# vis_data.head()\n",
    "# vis_data.plot.bar(y=1)\n",
    "vis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE NUMBER OF ONTOLOGIES TO BE IN CHART!!!\n",
    "num_ontologies = 20\n",
    "\n",
    "# Ceate a 'long' chart of the form [CellID | Annotation type | Annotation Value] (and probably UMAPs...)\n",
    "annotation_play = top_annotations\n",
    "annotation_play['ids'] = annotation_play.index\n",
    "vals = np.arange(0,num_ontologies)\n",
    "annotation_play = pd.melt(top_annotations, id_vars=['ids'], value_vars=vals, var_name = 'ontology')\n",
    "annotation_play['UMAP x'] = ''\n",
    "annotation_play['UMAP y'] = ''\n",
    "annotation_play['Cluster'] = ''\n",
    "\n",
    "## ADD THE UMAP LOCATIONS AND CLUSTER ID FOR EACH INSTANCE OF EACH CELL:\n",
    "# PS: Kinda hacked this one together.... so it takes a quick sec to run\n",
    "cnt = 1\n",
    "for i in range(len(annotation_play)):\n",
    "    for j in range(len(vis_data)):\n",
    "        if annotation_play.loc[i, 'ids'] == vis_data.loc[j,'IDs']:\n",
    "            annotation_play.loc[i, 'UMAP x'] = float(vis_data.loc[j,'UMAP x'])\n",
    "            annotation_play.loc[i, 'UMAP y'] = float(vis_data.loc[j,'UMAP y'])\n",
    "            annotation_play.loc[i, 'Cluster'] = int(vis_data.loc[j,'Clusters'])\n",
    "#             print(len(annotation_play) - cnt)\n",
    "            cnt += 1\n",
    "\n",
    "annotation_play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'master' plot  \n",
    "**TODO** \n",
    " - Add cluster colors\n",
    " - Add IDs when hovering?\n",
    " - Add ROI selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create histogram plot of ontologies:\n",
    "**Thoughts:**\n",
    " - Altair can plot more than the 'maxrows' limit, although it becomes very sluggish. They recommend to save the data locally as JSON and reference a path to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels dataframe:\n",
    "labels.tolist()\n",
    "df_labels = pd.DataFrame(labels.tolist(), columns=['labels'])\n",
    "df_labels = df_labels.loc[:num_ontologies,:]\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display tool!\n",
    "**This is where our interaction is!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a local JSON format:\n",
    "data_JSON = alt.data_transformers.enable('json')\n",
    "# Turn off max_rows limit -- this makes the visualization clunky\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "ROI = alt.selection_interval()\n",
    "click = alt.selection_multi()\n",
    "\n",
    "scatter_base = alt.Chart(annotation_play).mark_circle().encode(\n",
    "    x='UMAP x',\n",
    "    y='UMAP y',\n",
    "    color = alt.Color('Cluster:N', scale=alt.Scale(scheme = 'set1'))\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ").add_selection(ROI, click)\n",
    "\n",
    "br_sz = 20\n",
    "hist_base = alt.Chart(annotation_play).mark_bar(size=br_sz).encode(\n",
    "    x='ontology',\n",
    "    y='mean(value):Q'\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ")\n",
    "\n",
    "hist_roi = alt.Chart(annotation_play).mark_bar(size=br_sz, opacity=0.5).encode(\n",
    "    alt.X('ontology'),\n",
    "    alt.Y('mean(value):Q',\n",
    "          scale=alt.Scale(domain=(0,1))),\n",
    "    color=alt.value('red')\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ").transform_filter(\n",
    "    ROI | click\n",
    ")\n",
    "\n",
    "# hist_labels = alt.Chart(df_labels).mark_text(align='center', baseline='bottom',\n",
    "#                                  dy=35, fontSize=12\n",
    "#                                  ).encode(\n",
    "# y=alt.value(100),\n",
    "# text='labels:N')\n",
    "\n",
    "\n",
    "scatter_base & (hist_base + hist_roi)\n",
    "# hist_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is unpublished data from the Gagnon lab from the [cell ranger](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger) pipeline. I loaded the files, cleaned the data and created a (large) csv file to load in.  As discussed in peer feedback, the unpublished data will be shared will a Google Drive link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testis1_data_ = io.mmread('matrix.mtx.gz')\n",
    "#data_arr = testis1_data_.toarray()\n",
    "#data_col = pd.read_csv('barcodes.tsv.gz', compression = 'gzip', header=None, sep = '\\t')\n",
    "#data_row = pd.read_csv('features.tsv.gz', compression = 'gzip', header=None,sep = '\\t')\n",
    "#data_rows = data_row[1]\n",
    "#data_cols = []\n",
    "#for b in data_col[0]:\n",
    "#    data_cols.append(b)\n",
    "#data_counts = pd.DataFrame(data_arr, index = data_rows, columns = data_cols)\n",
    "#data_counts.to_csv('data_counts_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the link to the csv file I generated with the code above: [t1_counts_data.csv](https://drive.google.com/file/d/1aIR4w9TIOnMxziE5aQVjIMAWvqEY6Mvq/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The csv file above needs to be downloaded to load this cell\n",
    "# It is a large dataset, so it may take a while to load\n",
    "data_counts = pd.read_csv('t1_counts_data.csv', header = 0)\n",
    "data_counts.set_index('1', inplace = True)\n",
    "data_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality control of the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# percentage of mitochondrial rna\n",
    "mitochondrial = data_counts[data_counts.index.str.contains('mt-') > 0].sum(0)\n",
    "total = data_counts.sum(0)\n",
    "mito_percentage = (mitochondrial/total) * 100\n",
    "filter_out = mito_percentage[mito_percentage < 5].index\n",
    "data_counts = data_counts[filter_out] # filter out anything higher than 5%\n",
    "\n",
    "# filter by counts of genes (a very large amount of transcribed genes implies more than one cell is captured)\n",
    "gene_counts = []\n",
    "for i,cell in enumerate(data_counts.columns):\n",
    "    counter = 0\n",
    "    for row in data_counts.iloc[:,i]:\n",
    "        if row > 0:\n",
    "            counter += 1\n",
    "    gene_counts.append(counter)\n",
    "count_mask = []\n",
    "for count in gene_counts:\n",
    "    if count > 200 & count < 2500:\n",
    "        count_mask.append(gene_counts.index(count))\n",
    "data_counts = data_counts.iloc[:,count_mask]\n",
    "data_counts.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = np.arange(0,8627)\n",
    "mask = np.ones((len(indicies),1))\n",
    "for i in range(len(mask)):\n",
    "    if indicies[i] not in count_mask:\n",
    "        mask[i] = 0\n",
    "        print('count')\n",
    "    elif indicies[i] not in filter_out:\n",
    "        mask[i] = 0\n",
    "        print('indicies')\n",
    "sum(mask==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define features (genes) and samples (cell_ids)\n",
    "genes = data_counts.index.values\n",
    "cell_ids = data_counts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_genes = pd.read_csv('top_genes.csv')\n",
    "dc2 = data_counts.transpose()\n",
    "dc2[top_genes['0'].drop_duplicates()].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the same UMAP pipeline as above with new data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline for finding neighbors:**\n",
    " - Set test and train datasets\n",
    " - Set a list of k's like [k for k in range(2,10)] or something like that\n",
    " - Build loops for k's that sets the k-nn classifier as knn_model, then use knn_model.fit for the data, set y_pred with     knn_model.predict(X_test) and use the metrics.accuracy_score(y_true=y_test, y_pred=y_pred) for scores for each k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vals = data_counts.transpose().values\n",
    "# norm_vals = dc2.values\n",
    "norm_vals_scaled = StandardScaler().fit_transform(norm_vals)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for development purposes:\n",
    "X_train, X_test = train_test_split(norm_vals_scaled, test_size=0.75, random_state=1) #Reduce the number of cells for the clustering:\n",
    "print('done')\n",
    "# UMAP_train = umap.UMAP(n_neighbors = 7, min_dist = 0.2, n_components = 2).fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the UMAP parameters:\n",
    "**We** manually ran the two boxes below with varying parameters and visually assessed the results to identify our 'best' parameters for the UMAP reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UMAP_mod = umap.UMAP(n_neighbors = 13, min_dist = .5, n_components = 2).fit_transform(X_train)\n",
    "print('done')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1])\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_csv(numpy_array, write_name):\n",
    "    df_temp = pd.DataFrame(numpy_array)\n",
    "    df_temp.to_csv(write_name)\n",
    "    print('Successfully wrote array as ' + write_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize kmeans clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''n_rng = np.arange(3,9)\n",
    "score = []\n",
    "for n in n_rng:\n",
    "    print('Working on: %d' % n)\n",
    "    mdl = KMeans(n_clusters = 12, max_iter=10, random_state=1, ).fit(X_train)\n",
    "    score.append(metrics.silhouette_score(x_train, mdl.labels_, metric='euclidean'))\n",
    "print(n_rng)\n",
    "print(score)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the UMAP with the final parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UMAP_mod = umap.UMAP(n_neighbors = 26, min_dist = 0.5, n_components = 2).fit_transform(norm_vals_scaled) #Double the number of neighbors bec. we double sz of data\n",
    "# ^ UNCOMMENT TO RUN! IT TAKES FOREVER\n",
    "\n",
    "plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1])\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;\n",
    "\n",
    "np_to_csv(UMAP_mod, 'Gagnon_UMAP.csv')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the final clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''y_pred = KMeans(n_clusters=7, max_iter=5).fit_predict(norm_vals_scaled)# UNCOMMENT IF YOU WANT TO RUN! IT TAKES FOREVER!\n",
    "np_to_csv(y_pred, 'Gagnon_kMeans.csv') '''# Save the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the final results of the UMAP reduction with clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('Gagnon_UMAP.csv')\n",
    "UMAP_mod = df_temp[['0','1']].values\n",
    "plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1], c = y_pred, cmap = colors, s=15)\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-create the visualization pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = pd.read_csv('GeneAnnotation_gagnon.csv')\n",
    "annotation.rename(columns={'Unnamed: 0':'index'}, inplace=True)\n",
    "annotation.set_index('index',inplace=True)\n",
    "annot_vals = annotation.values\n",
    "annot_scaled = StandardScaler().fit_transform(annot_vals)\n",
    "\n",
    "# PCA on gene ontologies: Use this to figure out het\n",
    "pca_mod = PCA(n_components = 1) \n",
    "data_pca = pca_mod.fit_transform(annot_scaled)\n",
    "PCs =pca_mod.components_\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs_df = pd.DataFrame(PCs)\n",
    "PCs_df = PCs_df.transpose()\n",
    "PCs_df[1] = abs(PCs_df[0])\n",
    "topGo = PCs_df[1].sort_values(ascending = False).head(100).index #List of the index of the top gene ontologies\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_annotations = annotation.iloc[:,topGo] # Grab the top 100 annotations\n",
    "labels=top_annotations.columns\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "top_annotations[:] = min_max_scaler.fit_transform(top_annotations.values) #Scale the data to be in range [0,1]\n",
    "top_annotations.columns = np.arange(0,len(top_annotations.columns))\n",
    "top_annotations.head() #df with only the top 100 most important group ontologies listed for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final viz dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'base' dataframe:\n",
    "column_names = [\"UMAP x\",\"UMAP y\"]\n",
    "vis_data = pd.DataFrame(UMAP_mod, columns = column_names)\n",
    "vis_data[\"Clusters\"] = y_pred # Add clusters:\n",
    "vis_data['IDs'] = top_annotations.index.values\n",
    "# vis_data[top_annotations.columns] = top_annotations.values # Bring in the 'top annotations'\n",
    "\n",
    "# vis_data.head()\n",
    "# vis_data.plot.bar(y=1)\n",
    "vis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the data into a 'long-format' dataframe appropriate for altair**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE NUMBER OF ONTOLOGIES TO BE IN CHART!!!\n",
    "num_ontologies = 10\n",
    "\n",
    "# Ceate a 'long' chart of the form [CellID | Annotation type | Annotation Value] (and probably UMAPs...)\n",
    "annotation_play = top_annotations\n",
    "annotation_play['ids'] = annotation_play.index\n",
    "vals = np.arange(0,num_ontologies)\n",
    "annotation_play = pd.melt(top_annotations, id_vars=['ids'], value_vars=vals, var_name = 'ontology')\n",
    "annotation_play['UMAP x'] = ''\n",
    "annotation_play['UMAP y'] = ''\n",
    "annotation_play['Cluster'] = ''\n",
    "\n",
    "## ADD THE UMAP LOCATIONS AND CLUSTER ID FOR EACH INSTANCE OF EACH CELL:\n",
    "# PS: Kinda hacked this one together.... so it takes a quick sec to run\n",
    "cnt = 1\n",
    "for i in range(len(annotation_play)):\n",
    "    for j in range(len(vis_data)):\n",
    "        if annotation_play.loc[i, 'ids'] == vis_data.loc[j,'IDs']:\n",
    "            annotation_play.loc[i, 'UMAP x'] = float(vis_data.loc[j,'UMAP x'])\n",
    "            annotation_play.loc[i, 'UMAP y'] = float(vis_data.loc[j,'UMAP y'])\n",
    "            annotation_play.loc[i, 'Cluster'] = int(vis_data.loc[j,'Clusters'])\n",
    "#             print(len(annotation_play) - cnt)\n",
    "            cnt += 1\n",
    "\n",
    "annotation_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a local JSON format:\n",
    "data_JSON = alt.data_transformers.enable('json')\n",
    "# Turn off max_rows limit -- this makes the visualization clunky\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "ROI = alt.selection_interval()\n",
    "click = alt.selection_multi()\n",
    "\n",
    "scatter_base = alt.Chart(annotation_play).mark_circle().encode(\n",
    "    x='UMAP x',\n",
    "    y='UMAP y',\n",
    "    color = alt.Color('Cluster:N', scale=alt.Scale(scheme = 'set1'))\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ").add_selection(ROI, click)\n",
    "\n",
    "br_sz = 20\n",
    "hist_base = alt.Chart(annotation_play).mark_bar(size=br_sz).encode(\n",
    "    x='ontology',\n",
    "    y='mean(value):Q'\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ")\n",
    "\n",
    "hist_roi = alt.Chart(annotation_play).mark_bar(size=br_sz, opacity=0.5).encode(\n",
    "    alt.X('ontology'),\n",
    "    alt.Y('mean(value):Q',\n",
    "          scale=alt.Scale(domain=(0,1))),\n",
    "    color=alt.value('red')\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ").transform_filter(\n",
    "    ROI | click\n",
    ")\n",
    "\n",
    "# hist_labels = alt.Chart(df_labels).mark_text(align='center', baseline='bottom',\n",
    "#                                  dy=35, fontSize=12\n",
    "#                                  ).encode(\n",
    "# y=alt.value(100),\n",
    "# text='labels:N')\n",
    "\n",
    "\n",
    "scatter_base & (hist_base + hist_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
