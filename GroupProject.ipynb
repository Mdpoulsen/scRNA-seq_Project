{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we have done:  \n",
    "\n",
    "We downloaded the genetic data set found [here](https://www.ebi.ac.uk/gxa/sc/experiments/E-GEOD-100911/results?colourBy=metadata&metadata=genotype), and then loaded the data set into a dataframe. We then used the [Entrez API](https://www.ncbi.nlm.nih.gov/books/NBK25501/) to convert the gene transcript IDs to KEGG IDs for easier interpretation. As the Entrez API has a low request frequency limit, we collected the gene IDs and saved them in a csv file. With our dataset downloaded and cleaned, we began exploring the dataset more thoroughly. To date we have downloaded and integrated the [UMAP learn](https://umap-learn.readthedocs.io/en/latest/) toolkit into our scikit learn toolkits. We have then begun to cluster the data based on the UMAP-reduced dimensions.\n",
    "\n",
    "For further exploration, we have also explored the gene ontologies, specifically with the biological function, and have grouped genes associated with similar functions, and have mapped the ontologies to the cells in the genetic dataset. We hope to use these related functions to explore the idea of reducing multiple genes responsible for the same function into a single representative gene.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peer feedback:  \n",
    "#### Bernard Li:  \n",
    " - Go into more detail for the UMAP method.\n",
    "  - UMAP will be used for dimension reduction.\n",
    " - Are we going to use different methods?\n",
    "  - We’ll use UMAP for the reduction to work with the complexity of the data and  use additional methods for clustering.\n",
    " - The scope of the data will need a lot of processing to make the data reworkable.\n",
    "  - We have already set up an annotation dataframe with information on the genes, but we are dropping the information about the pathways and instead focusing on the gene ontology processes for the genes expressed in a cell.\n",
    " - Discuss availability of lab data for the ethics section.\n",
    "  - The lab data is unpublished, so it'll be shared through Google Drive.\n",
    " - Are we using the same species for the toy dataset and full dataset?\n",
    "  - Yes, both the toy dataset and the full dataset are from zebrafish.\n",
    " - For the purposes of this project, clarify the scope of the project for the audience or just go in a direction like following one gene for a demonstration.\n",
    "  - We’ll cluster the cells into cell ‘types’, and then run linear regression to compare the different states, and then display annotations such as function and gene ontology for the top relevant genes.\n",
    "\n",
    "#### Lam Nguyen:\n",
    " - Do we have a clear objective or just the tool?\n",
    "  - We need to focus a little more on a specific analysis instead of the features of the tool.\n",
    " - What are we running linear regression on?\n",
    "  - We were originally planning on running linear regression between the different cell clusters to compare the gene expression of each differentiation state.\n",
    " - Visualization tool drives the analysis, but it's more of an exploration.  Add more analysis to really improve the project.\n",
    "  - We'll focus on comparing the gene ontology processes between clusters of cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planned deviations from initial proposal:  \n",
    "As was suggested by our insightful peer reviewers, Bernard and Lam, we had an ambitious agenda for our project. Nearly halfway through the timeline of our project, we are realizing that we were overly vague in what we plan to accomplish within our time window. As such, we have decided to focus our efforts on creating an interactively-generated histogram of gene ontologies. More specifically, we will reduce our genetic data to two dimensions using UMAP and plot our cells in 2-D. We will cluster the cells into various differentiation steps, and then invite the user to select a region of interest within or across these clusters. Cells within this region of interest will be automatically analyzed and a histogram of the top ten most common gene ontologies will be generated. In practice, this allows the user to select a region of interest and rapidly identify primary shared biological traits between the cells, providing a platform for researchers to quickly understand the biological processes that drive cell differentiation at any point along the differentiation continuum. \n",
    "  \n",
    "With a greater focus on gene ontology, we plan to eliminate several features previously proposed for our tool. First, we will be visualizing the data in two-dimensions. This aids in selecting a region of interest. Second, we will eliminate the ability for the user to interactively choose the parameters for the UMAP reduction. We hope that a more focused purpose for our tool will more than offset the deviations from the original proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "import pandas as pd\n",
    "import umap # First time you run this enter pip install umap-learn in your Anaconda Prompt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining data and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtx_to_df (mtf, columns_file, rows_file):\n",
    "    \"\"\"Takes and mtf file path, a column file path and a row file path of an mtf file and returns a pandas dataframe\"\"\"\n",
    "    import pandas as pd\n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = io.mmread(mtf)\n",
    "    with open(columns_file) as file:\n",
    "        for line in file:\n",
    "            cols.append(line.rstrip())\n",
    "    \n",
    "    with open(rows_file) as file:\n",
    "        for line in file:\n",
    "            rows.append(line.rstrip().split('\\t')[0])\n",
    "    arr = data.toarray()\n",
    "    pd.DataFrame(arr, index = rows, columns = cols).to_csv('norm_counts_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = 'E-GEOD-100911.aggregated_filtered_normalised_counts.mtx'\n",
    "c = 'E-GEOD-100911.aggregated_filtered_normalised_counts.mtx_cols'\n",
    "r = 'E-GEOD-100911.aggregated_filtered_normalised_counts.mtx_rows'\n",
    "\n",
    "#mtx_to_df(norm,c,r)\n",
    "norm_counts = pd.read_csv('norm_counts_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was run to convert the gene transcript IDs to KEGG IDs, since this process took so long\n",
    "# the result was saved to a csv (geneID.csv) file and this code is not needed anymore. \n",
    "\n",
    "# from Bio import Entrez\n",
    "# import time\n",
    "# import csv\n",
    "\n",
    "# Entrez.email = \"mdpouls1@gmail.com\"\n",
    "# geneIDs = []\n",
    "# rowsTest = rows[15000:]\n",
    "# i = 0\n",
    "# while i < len(rowsTest):\n",
    "#     for t in range(3):\n",
    "#         if i < len(rowsTest):\n",
    "#             handle = Entrez.esearch(db=\"gene\", term=rowsTest[i])\n",
    "#             record = Entrez.read(handle)\n",
    "#             if len(record['IdList']) == 1:\n",
    "#                 geneIDs.append(record['IdList'][0])\n",
    "#             elif len(record['IdList']) > 1:\n",
    "#                 geneIDs.append('multiple')\n",
    "#             else:\n",
    "#                 geneIDs.append('NaN')\n",
    "#             handle.close()\n",
    "#             i += 1\n",
    "#     time.sleep(1)\n",
    "\n",
    "# with open('geneID.csv', 'a') as f:\n",
    "#     for gene in geneIDs:\n",
    "#         f.write('{},'.format(gene))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnnotations(uniprotData):\n",
    "    \"\"\"Takes the uniprotData file and creates a csv file with the gene ontologys (biological process) for each gene.\n",
    "    uniprot chart must include a gene ontology (biological process) column and either a Ids column or Gene names column. \n",
    "    The created csv has the gene ontologies as  columns and genes as rows with a 0 if the gene does not contain the GO term \n",
    "    and a 1 if it does\"\"\"\n",
    "    \n",
    "    #read the csv into a dataframe\n",
    "    annotationData = pd.read_table(uniprotData)\n",
    "    \n",
    "    #get a list of all the used GO terms in the dataset\n",
    "    GO = []\n",
    "    go_dict = {}\n",
    "    for line in annotationData['Gene ontology (biological process)'].str.split(';'):\n",
    "        if type(line) == list:\n",
    "            for item in line:\n",
    "                GO.append(item.strip())\n",
    "\n",
    "    #create a dictionary with an empty list for each GO term\n",
    "    for term in GO:\n",
    "        if term not in go_dict:\n",
    "            go_dict[term] = []\n",
    "\n",
    "    #check each gene \n",
    "    for line in annotationData['Gene ontology (biological process)'].str.split(';'):\n",
    "        item_dict = {}\n",
    "        if type(line) == list:\n",
    "            for item in line:\n",
    "                item_dict[item.strip()] = True\n",
    "            for k,v in go_dict.items():\n",
    "                if k in item_dict:\n",
    "                    v.append(1)\n",
    "                else:\n",
    "                    v.append(0)\n",
    "\n",
    "\n",
    "    go_df = pd.DataFrame.from_dict(go_dict)\n",
    "    \n",
    "    if \"Ids\" in annotationData.columns:\n",
    "        go_df['geneId'] = annotationData['Ids']\n",
    "        go_df = go_df.set_index('geneId')\n",
    "    \n",
    "    if 'Gene names' in annotationData.columns:\n",
    "        go_df['geneName'] = annotationData['Gene names']\n",
    "        go_df = go_df.drop_duplicates(subset = 'geneName')\n",
    "        go_df = go_df.set_index('geneName')\n",
    "\n",
    "    go_df.to_csv('goData.csv')\n",
    "    return(go_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCellGOTerms(norm_counts,GO_dataframe, filepathName):\n",
    "    \"\"\"Takes the normalized counts as a dataframe of normalized counts with cell IDs as the columns and gene names as the rows.\n",
    "    The normalized counts dataframe must have the column with the gene IDs named '1'. the GO_dataframe parameter is the output \n",
    "    from the getAnnotations function. The filepathName is the name of the file you want the csv to be written to\n",
    "    \n",
    "    Writes a csv of the relative amounts of a gene ontology term a cell has based off the differentially expressed genes. The\n",
    "    csv has the GO term as colums and the cell ID as rows.\"\"\"\n",
    "    \n",
    "    norm_counts_id = norm_counts.set_index(\"1\")\n",
    "    srr_dict = norm_counts_id.to_dict()\n",
    "\n",
    "\n",
    "    #Convert the go_df datafram into a dictionary of lists for the indices, columns, and data\n",
    "    gene_dict = GO_dataframe.to_dict('split')\n",
    "\n",
    "    #create a dictionary with the gene ID as the key and a list of the gene ontologies for that gene\n",
    "    geneID_dict = {}\n",
    "    for ind, data in gene_dict.items():\n",
    "        geneID_dict[ind] = data\n",
    "\n",
    "\n",
    "    ids = gene_dict['index']\n",
    "    data = gene_dict['data']\n",
    "\n",
    "    geneID_dict = {}\n",
    "    for i in range(len(ids)):\n",
    "        geneID_dict[ids[i]] = data[i]\n",
    "\n",
    "    srr_genes_dict_values = {}\n",
    "    for srr,genes in srr_dict.items():\n",
    "        genes_GO = np.zeros((len(GO_dataframe.columns),), dtype=int)\n",
    "        for gene,value in genes.items():\n",
    "            if value > 0:\n",
    "                if gene in geneID_dict:\n",
    "                    genes_GO = np.add(genes_GO,np.multiply(value, np.array(geneID_dict[gene])))\n",
    "        srr_genes_dict_values[srr] = genes_GO\n",
    "\n",
    "    #convert to a dataframe\n",
    "    cols = GO_dataframe.columns\n",
    "    gene_annotation_values_df = pd.DataFrame.from_dict(srr_genes_dict_values, orient='index', columns = cols)\n",
    "    #write data to a csv file\n",
    "    gene_annotation_values_df.to_csv('{}.csv'.format(filepathName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gagnonData_path = \"uniprot_gagnon.tab.gz\"\n",
    "# toyData_path = \"uniprotData.tab\"\n",
    "\n",
    "# testis = getAnnotations(gagnonData_path)\n",
    "# kidney = getAnnotations(toyData_path)\n",
    "\n",
    "# getCellGOTerms(norm_counts,testis,'GeneAnnotation_gagnon')\n",
    "# getCellGOTerms(norm_counts,kidney,'GeneAnnotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_counts = norm_counts.set_index('Unnamed: 0')\n",
    "norm_transpose = norm_counts.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP Reduction:\n",
    "\n",
    "See the UMAP Documentation page for details. The module import cell at the top of this notebook has instructions for downloading the scikit learn plug-in for UMAP.\n",
    "\n",
    "NOTE This is a first pass - I'm not super sure it is ready to go. Next steps\n",
    "\n",
    "1 We need to cluster the data and assign tags to the data points\n",
    "\n",
    "2 Add a third dimension to the UMAP reduction\n",
    "\n",
    "3 Plot data in altair to allow interaction\n",
    "\n",
    "4 Add interaction steps, tags, whatever else we're doing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_counts_temp = norm_counts\n",
    "norm_counts_temp = norm_counts_temp.drop(['geneIDs'],axis=1).transpose()\n",
    "norm_vals = norm_counts_temp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vals_scaled = StandardScaler().fit_transform(norm_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_reducer = umap.UMAP()\n",
    "reduced_genes = UMAP_reducer.fit_transform(norm_vals_scaled)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(reduced_genes[:,0],reduced_genes[:,1])\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = norm_vals_scaled\n",
    "# PCA\n",
    "pca_mod = PCA(n_components = 7)\n",
    "data_pca = pca_mod.fit_transform(X)\n",
    "PCs =pca_mod.components_\n",
    "PCs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_mod = umap.UMAP(n_neighbors = 7, min_dist = 0.2, n_components = 3).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means\n",
    "y_pred = KMeans(n_clusters=7, max_iter=5).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors\n",
    "colors = ListedColormap(sns.color_palette('bright', 7).as_hex())\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(UMAP_mod[:, 0], UMAP_mod[:, 1],zs= UMAP_mod[:, 2], c=y_pred, cmap= colors, s=20)\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1], c = y_pred, cmap = colors, s=15)\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top gene ontologies:  \n",
    "**The** top gene ontologies are defined in this project as those that have the largest (absolute) parameter value in the final equation for the principal direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = pd.read_csv('GeneAnnotation.csv')\n",
    "annot_vals = annotation.values\n",
    "\n",
    "annot_scaled = StandardScaler().fit_transform(annot_vals)\n",
    "\n",
    "# PCA on gene ontologies: Use this to figure out het\n",
    "pca_mod = PCA(n_components = 1) \n",
    "data_pca = pca_mod.fit_transform(annot_scaled)\n",
    "PCs =pca_mod.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs_df = pd.DataFrame(PCs)\n",
    "PCs_df = PCs_df.transpose()\n",
    "PCs_df[1] = abs(PCs_df[0])\n",
    "topGo = PCs_df[1].sort_values(ascending = False).head(100).index #List of the index of the top gene ontologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_annotations = annotation.iloc[:,topGo] # Grab the top 100 annotations\n",
    "labels=top_annotations.columns\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "top_annotations[:] = min_max_scaler.fit_transform(top_annotations.values) #Scale the data to be in range [0,1]\n",
    "top_annotations.columns = np.arange(0,len(top_annotations.columns))\n",
    "top_annotations.head() #df with only the top 100 most important group ontologies listed for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization:\n",
    "**Overview**\n",
    " - The 'base' plot is the 2-D UMAP plot plotted using Altair. \n",
    " - A user can then select a region-of-interest to create the breakout histogram plots of predominate gene ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create visualization dataframe:  \n",
    "**Overview** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'base' dataframe:\n",
    "column_names = [\"UMAP x\",\"UMAP y\"]\n",
    "vis_data = pd.DataFrame(reduced_genes, columns = column_names)\n",
    "vis_data[\"Clusters\"] = y_pred # Add clusters:\n",
    "vis_data['IDs'] = top_annotations.index.values\n",
    "# vis_data[top_annotations.columns] = top_annotations.values # Bring in the 'top annotations'\n",
    "\n",
    "# vis_data.head()\n",
    "# vis_data.plot.bar(y=1)\n",
    "vis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE NUMBER OF ONTOLOGIES TO BE IN CHART!!!\n",
    "num_ontologies = 20\n",
    "\n",
    "# Ceate a 'long' chart of the form [CellID | Annotation type | Annotation Value] (and probably UMAPs...)\n",
    "annotation_play = top_annotations\n",
    "annotation_play['ids'] = annotation_play.index\n",
    "vals = np.arange(0,num_ontologies)\n",
    "annotation_play = pd.melt(top_annotations, id_vars=['ids'], value_vars=vals, var_name = 'ontology')\n",
    "annotation_play['UMAP x'] = ''\n",
    "annotation_play['UMAP y'] = ''\n",
    "annotation_play['Cluster'] = ''\n",
    "\n",
    "## ADD THE UMAP LOCATIONS AND CLUSTER ID FOR EACH INSTANCE OF EACH CELL:\n",
    "# PS: Kinda hacked this one together.... so it takes a quick sec to run\n",
    "cnt = 1\n",
    "for i in range(len(annotation_play)):\n",
    "    for j in range(len(vis_data)):\n",
    "        if annotation_play.loc[i, 'ids'] == vis_data.loc[j,'IDs']:\n",
    "            annotation_play.loc[i, 'UMAP x'] = float(vis_data.loc[j,'UMAP x'])\n",
    "            annotation_play.loc[i, 'UMAP y'] = float(vis_data.loc[j,'UMAP y'])\n",
    "            annotation_play.loc[i, 'Cluster'] = int(vis_data.loc[j,'Clusters'])\n",
    "#             print(len(annotation_play) - cnt)\n",
    "            cnt += 1\n",
    "\n",
    "annotation_play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'master' plot  \n",
    "**TODO** \n",
    " - Add cluster colors\n",
    " - Add IDs when hovering?\n",
    " - Add ROI selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create histogram plot of ontologies:\n",
    "**Thoughts:**\n",
    " - Altair can plot more than the 'maxrows' limit, although it becomes very sluggish. They recommend to save the data locally as JSON and reference a path to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels dataframe:\n",
    "labels.tolist()\n",
    "df_labels = pd.DataFrame(labels.tolist(), columns=['labels'])\n",
    "df_labels = df_labels.loc[:num_ontologies,:]\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display tool!\n",
    "**This is where our interaction is!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save data to a local JSON format:\n",
    "data_JSON = alt.data_transformers.enable('json')\n",
    "# Turn off max_rows limit -- this makes the visualization clunky\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "ROI = alt.selection_interval()\n",
    "click = alt.selection_multi()\n",
    "\n",
    "scatter_base = alt.Chart(annotation_play).mark_circle().encode(\n",
    "    x='UMAP x',\n",
    "    y='UMAP y',\n",
    "    color = alt.Color('Cluster:N', scale=alt.Scale(scheme = 'set1'))\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ").add_selection(ROI, click)\n",
    "\n",
    "br_sz = 20\n",
    "hist_base = alt.Chart(annotation_play).mark_bar(size=br_sz).encode(\n",
    "    x='ontology',\n",
    "    y='mean(value):Q'\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ")\n",
    "\n",
    "hist_roi = alt.Chart(annotation_play).mark_bar(size=br_sz, opacity=0.5).encode(\n",
    "    alt.X('ontology'),\n",
    "    alt.Y('mean(value):Q',\n",
    "          scale=alt.Scale(domain=(0,1))),\n",
    "    color=alt.value('red')\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ").transform_filter(\n",
    "    ROI | click\n",
    ")\n",
    "\n",
    "# hist_labels = alt.Chart(df_labels).mark_text(align='center', baseline='bottom',\n",
    "#                                  dy=35, fontSize=12\n",
    "#                                  ).encode(\n",
    "# y=alt.value(100),\n",
    "# text='labels:N')\n",
    "\n",
    "\n",
    "scatter_base & (hist_base + hist_roi)\n",
    "# hist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show ontology labels for histogram above\n",
    "for i, ont in enumerate(df_labels['labels']):\n",
    "    print(i, ':',  ont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is unpublished data from the Gagnon lab from the [cell ranger](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger) pipeline. I loaded the files, cleaned the data and created a (large) csv file to load in.  As discussed in peer feedback, the unpublished data will be shared will a Google Drive link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testis1_data_ = io.mmread('matrix.mtx.gz')\n",
    "#data_arr = testis1_data_.toarray()\n",
    "#data_col = pd.read_csv('barcodes.tsv.gz', compression = 'gzip', header=None, sep = '\\t')\n",
    "#data_row = pd.read_csv('features.tsv.gz', compression = 'gzip', header=None,sep = '\\t')\n",
    "#data_rows = data_row[1]\n",
    "#data_cols = []\n",
    "#for b in data_col[0]:\n",
    "#    data_cols.append(b)\n",
    "#data_counts = pd.DataFrame(data_arr, index = data_rows, columns = data_cols)\n",
    "#data_counts.to_csv('data_counts_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the link to the csv file I generated with the code above: [t1_counts_data.csv](https://drive.google.com/file/d/1aIR4w9TIOnMxziE5aQVjIMAWvqEY6Mvq/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The csv file above needs to be downloaded to load this cell\n",
    "# It is a large dataset, so it may take a while to load\n",
    "data_counts = pd.read_csv('t1_counts_data.csv', header = 0)\n",
    "data_counts.set_index('1', inplace = True)\n",
    "X = data_counts\n",
    "data_counts.shape # original size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality control of the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# percentage of mitochondrial rna\n",
    "mitochondrial = data_counts[data_counts.index.str.contains('mt-') > 0].sum(0)\n",
    "total = data_counts.sum(0)\n",
    "mito_percentage = (mitochondrial/total) * 100\n",
    "filter_out = mito_percentage[mito_percentage < 5].index\n",
    "data_counts = data_counts[filter_out] # filter out anything higher than 5%\n",
    "\n",
    "# filter by counts of genes (a very large amount of transcribed genes implies more than one cell is captured)\n",
    "gene_counts = []\n",
    "for i,cell in enumerate(data_counts.columns):\n",
    "    counter = 0\n",
    "    for row in data_counts.iloc[:,i]:\n",
    "        if row > 0:\n",
    "            counter += 1\n",
    "    gene_counts.append(counter)\n",
    "count_mask = []\n",
    "for count in gene_counts:\n",
    "    if count > 200 & count < 2500:\n",
    "        count_mask.append(gene_counts.index(count))\n",
    "data_counts = data_counts.iloc[:,count_mask]\n",
    "data_counts.shape # cutting down cells check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (genes) and samples (cell_ids)\n",
    "genes = data_counts.index.values\n",
    "cell_ids = data_counts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indicies = np.arange(0,8627)\n",
    "mask = np.ones((len(indicies),1))\n",
    "for i in range(len(mask)):\n",
    "    if indicies[i] not in count_mask:\n",
    "        mask[i] = 0\n",
    "        #print('count')\n",
    "    elif indicies[i] not in filter_out:\n",
    "        mask[i] = 0\n",
    "        #print('indicies')\n",
    "sum(mask==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale data to get mean-variance standardized values\n",
    "X = data_counts.to_numpy()\n",
    "ss_mod = StandardScaler()\n",
    "X = ss_mod.fit_transform(X)\n",
    "# ^ these values are just mean-variance standarized, use the variance for each gene and filter out highest (2,000 is recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find variances of each gene and sort for top variable genes\n",
    "gene_variances = np.var(X, axis=1)\n",
    "top_variable_genes = pd.DataFrame(gene_variances, index=genes, columns=['variances'])\n",
    "top_variable_genes['inds'] = [i for i in range(0,32520)]\n",
    "tvg = top_variable_genes.sort_values('variances', ascending=False)[:2000] # Top Variable Genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing duplicates by index comparisons (gene name leads to doubles) and subsetting original data_counts\n",
    "tvg_inds = tvg['inds']\n",
    "tvgi = tvg.index.values\n",
    "tvgidf = pd.DataFrame(tvgi)\n",
    "# save csv\n",
    "# tvgidf.to_csv('top_genes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually troubleshooting gene names\n",
    "tdc = data_counts[data_counts.index.isin(tvgidf.iloc[:,0])].drop_duplicates() #top data_counts with duplicates\n",
    "tdci = tdc.index.values # #top data_counts genes\n",
    "tdci.sort() # Alphabetically orders both so that they can be compared \n",
    "tvgi.sort() # to the top variable genes list\n",
    "tdcil = tdci.tolist()\n",
    "# troubleshooting dataset doubles, rm holds the dropped indexs for the duplicates\n",
    "rm = [122,213, 250, 298, 437, 561,637, 700, 701, 706, 730, 976, 1339, 1620, 1685, 1766, 1846, 1972, 1981]\n",
    "# uncomment prints to see duplicates\n",
    "for i in rm:\n",
    "    if tvgi[i] == tdci[i]:\n",
    "            print('Clear')\n",
    "    elif i != 701:\n",
    "            #print(i-1, tvgi[i-1], tdci[i-1])\n",
    "           # print('===',i,tvgi[i], tdci[i])\n",
    "           # print(i+1,tvgi[i+1], tdci[i+1])\n",
    "            tdcil.remove(tdci[i])\n",
    "            tdci = np.asarray(tdcil)\n",
    "           # print('AFTER')\n",
    "           # print(i-1, tvgi[i-1], tdci[i-1])\n",
    "           # print('===',i,tvgi[i], tdci[i])\n",
    "           # print(i+1,tvgi[i+1], tdci[i+1])\n",
    "            #print()\n",
    "    elif i == 701:\n",
    "           # print(i-1, tvgi[i-1], tdci[i-1])\n",
    "           # print('===',i,tvgi[i], tdci[i])\n",
    "           # print(i+1,tvgi[i+1], tdci[i+1])\n",
    "            tdcil.remove(tdci[i-1]) # shift index\n",
    "            tdci = np.asarray(tdcil)\n",
    "            #print('AFTER:')\n",
    "            #print(i-1, tvgi[i-1], tdci[i-1])\n",
    "            # print('===',i,tvgi[i], tdci[i])\n",
    "            #print(i+1,tvgi[i+1], tdci[i+1])\n",
    "            #print()\n",
    "print(len(tdci))\n",
    "\n",
    "# checking datasets\n",
    "for i in range(0,2000):\n",
    "    if tvgi[i] != tdci[i]: # they are the same if the ERROR isn't printed\n",
    "        print('ERROR:', i, tvgi[i], tdci[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the highest variable genes, there were multiple genes that had more than one row in data_counts, so: this section puts the highest expression (closer to a large variability score) in and adding to the list of rows to subset from the original 32,520 genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CAN ONLY RUN ONCE\n",
    "dc_inds = [] # original indicies from data_counts to reference\n",
    "indices = [] # Subsetting mask\n",
    "dupls = [] # list of duplicates that aren't filtered out by dropping duplicates (.drop_duplicates() can't catch these because of genes with doubles)\n",
    "\n",
    "# add the indicies of genes in the original set\n",
    "for g in tdci: \n",
    "    dc_inds.append(top_variable_genes.loc[g, 'inds'])\n",
    "for i in dc_inds:\n",
    "    if type(i) == np.float64:\n",
    "        indices.append(int(i))\n",
    "    else:\n",
    "        dupls.append(i)\n",
    "\n",
    "# for the duplicates compare the rows of duplicate data_counts (which are different), find the set with the higher expression\n",
    "for dup in dupls:\n",
    "    if dup.shape[0] == 2:\n",
    "        a = dup.iloc[0]\n",
    "        b = dup.iloc[1]\n",
    "        Aa = data_counts.iloc[a, :]\n",
    "        Ab = data_counts.iloc[b, :]\n",
    "        AA = Aa.compare(Ab)\n",
    "        asum = AA['self'].sum()\n",
    "        bsum = AA['other'].sum()\n",
    "        if asum > bsum:\n",
    "            if a not in indices:\n",
    "                indices.append(a)\n",
    "        elif b not in indices:\n",
    "            indices.append(b)\n",
    "    else: # this is for the triple duplicates which compares all three\n",
    "        a = dup.iloc[0]\n",
    "        b = dup.iloc[1]\n",
    "        Aa = data_counts.iloc[a, :]\n",
    "        Ab = data_counts.iloc[b, :]\n",
    "        AA = Aa.compare(Ab)\n",
    "        asum = AA['self'].sum()\n",
    "        bsum = AA['other'].sum()\n",
    "        if asum > bsum:\n",
    "            w = a\n",
    "            Ww = Aa\n",
    "            winsum = asum\n",
    "        else:\n",
    "            w = b\n",
    "            Ww = Ab\n",
    "            winsum = bsum\n",
    "        c = dup.iloc[2]\n",
    "        Ac = data_counts.iloc[c, :]\n",
    "        AB = Ww.compare(Ac)\n",
    "        csum = AB['other'].sum()\n",
    "        if csum > winsum:\n",
    "            if c not in indices:\n",
    "                indices.append(c)\n",
    "        elif w not in indices:\n",
    "            indices.append(w)\n",
    "indices.sort() # sort by order of indicies\n",
    "print(len(indices)) # check shape before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, subset with the indicies mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_counts = data_counts.iloc[indices, :] \n",
    "print(data_counts.shape) # check shape after to compare from before subsetting,\n",
    "# now it should show 1998 (2 of the duplicates had triples)\n",
    "\n",
    "# save csv\n",
    "#data_counts.to_csv('data_counts.csv')\n",
    "\n",
    "# Reset X from the original shape to the top variable genes subset\n",
    "X = data_counts.to_numpy()\n",
    "X.shape\n",
    "# Redefine features and samples\n",
    "genes = data_counts.index.values\n",
    "cell_ids = data_counts.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate descriptions for the top variable genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptions of genes  \n",
    "genes_desc = pd.read_csv('GENE-DESCRIPTION-TXT_ZFIN_17.txt', names=['gene'], skiprows=20, sep='\\t') #downloaded from: https://www.alliancegenome.org/downloads#gene-descriptions\n",
    "# gene descriptions (the _d descriptions and the _g genes)\n",
    "gd_d = genes_desc.index.values.tolist()\n",
    "gd_d = gd_d[1::2]\n",
    "gd_g = genes_desc.iloc[:,0].dropna().tolist()\n",
    "\n",
    "genes_descriptions = pd.DataFrame(gd_d, index = gd_g, columns = ['Description'])\n",
    "print(genes_descriptions.shape)\n",
    "\n",
    "gene_mask = genes.tolist()\n",
    "gm_inds = []\n",
    "for i,g in enumerate(genes_descriptions.index):\n",
    "    if g in gene_mask:\n",
    "        gm_inds.append(i)\n",
    "tvg_gd = genes_descriptions.iloc[gm_inds, :] # Subsetted gene descriptions\n",
    "tvg_gd.shape # only found descriptions for 1852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gnf_g = []\n",
    "for g in genes:\n",
    "    if g not in tvg_gd.index.values:\n",
    "        tvg_gd.loc[f'{g}'] = 'Gene not found'\n",
    "tvg_gd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvg_gd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_genes = pd.read_csv('top_genes.csv')\n",
    "dc2 = data_counts.transpose()\n",
    "dc2[top_genes['0'].drop_duplicates()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the same UMAP pipeline as above with new data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline for finding neighbors:**\n",
    " - Set test and train datasets\n",
    " - Set a list of k's like [k for k in range(2,10)] or something like that\n",
    " - Build loops for k's that sets the k-nn classifier as knn_model, then use knn_model.fit for the data, set y_pred with     knn_model.predict(X_test) and use the metrics.accuracy_score(y_true=y_test, y_pred=y_pred) for scores for each k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vals = data_counts.transpose().values\n",
    "# norm_vals = dc2.values\n",
    "norm_vals_scaled = StandardScaler().fit_transform(norm_vals)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for development purposes:\n",
    "X_train, X_test = train_test_split(norm_vals_scaled, test_size=0.75, random_state=1) #Reduce the number of cells for the clustering:\n",
    "print('done')\n",
    "# UMAP_train = umap.UMAP(n_neighbors = 7, min_dist = 0.2, n_components = 2).fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the UMAP parameters:\n",
    "**We** manually ran the two boxes below with varying parameters and visually assessed the results to identify our 'best' parameters for the UMAP reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''UMAP_mod = umap.UMAP(n_neighbors = 13, min_dist = .5, n_components = 2).fit_transform(X_train)\n",
    "print('done')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1])\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_csv(numpy_array, write_name):\n",
    "    df_temp = pd.DataFrame(numpy_array)\n",
    "    df_temp.to_csv(write_name)\n",
    "    print('Successfully wrote array as ' + write_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize kmeans clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''n_rng = np.arange(3,9)\n",
    "score = []\n",
    "for n in n_rng:\n",
    "    print('Working on: %d' % n)\n",
    "    mdl = KMeans(n_clusters = 12, max_iter=10, random_state=1, ).fit(X_train)\n",
    "    score.append(metrics.silhouette_score(x_train, mdl.labels_, metric='euclidean'))\n",
    "print(n_rng)\n",
    "print(score)''''''plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1])\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.show;'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the UMAP with the final parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_mod = umap.UMAP(n_neighbors = 27, min_dist = 0.70, n_components = 2, random_state=10).fit_transform(norm_vals_scaled) #Double the number of neighbors bec. we double sz of data\n",
    "# ^ UNCOMMENT TO RUN! IT TAKES FOREVER\n",
    "\n",
    "plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1])\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show;\n",
    "\n",
    "np_to_csv(UMAP_mod, 'Gagnon_UMAP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ks = range(1,21)\n",
    "scores = []\n",
    "for k in ks:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    y_pred = model.fit_predict(X)\n",
    "    scores.append(-model.score(X))\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "plt.plot(ks, scores)\n",
    "plt.ylabel('total intra-cluster distance')\n",
    "plt.xticks([i for i in range(1,20)])\n",
    "plt.xlabel('k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the final clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 12\n",
    "y_pred = KMeans(n_clusters=nc, max_iter=5).fit_predict(norm_vals_scaled)# UNCOMMENT IF YOU WANT TO RUN! IT TAKES FOREVER!\n",
    "np_to_csv(y_pred, 'Gagnon_kMeans.csv') # Save the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the final results of the UMAP reduction with clustering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ListedColormap(sns.color_palette('Paired', nc).as_hex())\n",
    "df_temp = pd.read_csv('Gagnon_UMAP.csv')\n",
    "UMAP_mod = df_temp[['0','1']].values\n",
    "scatter = plt.scatter(UMAP_mod[:,0],UMAP_mod[:,1], c = y_pred, cmap = colors, s=14)\n",
    "plt.gca().set_aspect('equal','datalim')\n",
    "\n",
    "plt.title('UMAP projection of Zebra Fish genes, unclustered')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.legend(*scatter.legend_elements())\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make empty containers with clusters\n",
    "def make_cl_cont(cs, listfill=False):\n",
    "    cl_dict = {}\n",
    "    for cl in cs:\n",
    "        if listfill:\n",
    "            cl_dict[cl] = []\n",
    "        else:\n",
    "            cl_dict[cl] = 0\n",
    "    return cl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset clusters\n",
    "cs = [f'cluster{i}' for i in range(0,nc)]\n",
    "cl_subs = make_cl_cont(cs, listfill=True)\n",
    "for i,cc in enumerate(y_pred):\n",
    "    cl_subs[f'cluster{cc}'].append(i)\n",
    "len(cl_subs)\n",
    "# generate sum for each gene for each cell in each cluster\n",
    "g_cls = make_cl_cont(cs)\n",
    "for cl in cl_subs:\n",
    "    cl_g_sums = []\n",
    "    cl_c = data_counts.iloc[:,cl_subs[cl]]\n",
    "    for row in cl_c.index:\n",
    "        cl_g_sums.append(cl_c.loc[row].sum())\n",
    "    g_cls[cl] = cl_g_sums\n",
    "tg_cls =make_cl_cont(cs)\n",
    "for cl in g_cls:\n",
    "    gene_df = pd.DataFrame(g_cls[cl], index=genes, columns=['cl_sums'])\n",
    "    gene_df = gene_df.sort_values('cl_sums', ascending=False) # sort from highest to lowest\n",
    "    tg_cls[cl] = gene_df.index.values[:25] # select top 25\n",
    "len(tg_cls)\n",
    "hm_inds = make_cl_cont(cs,listfill=True)\n",
    "for cl in tg_cls:\n",
    "    for g in tg_cls[cl]:\n",
    "        for i,row in enumerate(data_counts.index):\n",
    "            if row == g:\n",
    "                hm_inds[cl].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize Cluster and top expressed genes by input (run for each cluster)\n",
    "cl_v = input('Cluster number: ')\n",
    "plt.title(f'Cluster {cl_v} Top Expressed Genes')\n",
    "plt.xlabel('Cells')\n",
    "plt.ylabel('Genes')\n",
    "plt.tick_params(left=True, bottom=False)\n",
    "sns.heatmap(data_counts.iloc[hm_inds[f'cluster{cl_v}'], cl_subs[f'cluster{cl_v}']]);\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(tvg_gd.loc[tg_cls[f'cluster{cl_v}']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-create the visualization pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = pd.read_csv('GeneAnnotation_gagnon.csv')\n",
    "annotation.rename(columns={'Unnamed: 0':'index'}, inplace=True)\n",
    "annotation.set_index('index',inplace=True)\n",
    "annot_vals = annotation.values\n",
    "annot_scaled = StandardScaler().fit_transform(annot_vals)\n",
    "\n",
    "# PCA on gene ontologies: Use this to figure out het\n",
    "pca_mod = PCA(n_components = 1) \n",
    "data_pca = pca_mod.fit_transform(annot_scaled)\n",
    "PCs =pca_mod.components_\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs_df = pd.DataFrame(PCs)\n",
    "PCs_df = PCs_df.transpose()\n",
    "PCs_df[1] = abs(PCs_df[0])\n",
    "topGo = PCs_df[1].sort_values(ascending = False).head(100).index #List of the index of the top gene ontologies\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_annotations = annotation.iloc[:,topGo] # Grab the top 100 annotations\n",
    "labels=top_annotations.columns\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "top_annotations[:] = min_max_scaler.fit_transform(top_annotations.values) #Scale the data to be in range [0,1]\n",
    "top_annotations.columns = np.arange(0,len(top_annotations.columns))\n",
    "top_annotations.head() #df with only the top 100 most important group ontologies listed for each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final viz dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'base' dataframe:\n",
    "column_names = [\"UMAP x\",\"UMAP y\"]\n",
    "vis_data = pd.DataFrame(UMAP_mod, columns = column_names)\n",
    "vis_data[\"Clusters\"] = y_pred # Add clusters:\n",
    "\n",
    "vis_data['IDs'] = top_annotations.index.values\n",
    "# vis_data[top_annotations.columns] = top_annotations.values # Bring in the 'top annotations'\n",
    "\n",
    "# vis_data.head()\n",
    "# vis_data.plot.bar(y=1)\n",
    "vis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format the data into a 'long-format' dataframe appropriate for altair**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOOSE NUMBER OF ONTOLOGIES TO BE IN CHART!!!\n",
    "num_ontologies = 10\n",
    "\n",
    "# Ceate a 'long' chart of the form [CellID | Annotation type | Annotation Value] (and probably UMAPs...)\n",
    "annotation_play = top_annotations\n",
    "annotation_play['ids'] = annotation_play.index\n",
    "vals = np.arange(0,num_ontologies)\n",
    "annotation_play = pd.melt(top_annotations, id_vars=['ids'], value_vars=vals, var_name = 'ontology')\n",
    "annotation_play['UMAP x'] = ''\n",
    "annotation_play['UMAP y'] = ''\n",
    "annotation_play['Cluster'] = ''\n",
    "\n",
    "## ADD THE UMAP LOCATIONS AND CLUSTER ID FOR EACH INSTANCE OF EACH CELL:\n",
    "# PS: Kinda hacked this one together.... so it takes a quick sec to run\n",
    "cnt = 1\n",
    "for i in range(len(annotation_play)):\n",
    "    for j in range(len(vis_data)):\n",
    "        if annotation_play.loc[i, 'ids'] == vis_data.loc[j,'IDs']:\n",
    "            annotation_play.loc[i, 'UMAP x'] = float(vis_data.loc[j,'UMAP x'])\n",
    "            annotation_play.loc[i, 'UMAP y'] = float(vis_data.loc[j,'UMAP y'])\n",
    "            annotation_play.loc[i, 'Cluster'] = int(vis_data.loc[j,'Clusters'])\n",
    "#             print(len(annotation_play) - cnt)\n",
    "            cnt += 1\n",
    "\n",
    "annotation_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to a local JSON format:\n",
    "data_JSON = alt.data_transformers.enable('json')\n",
    "# Turn off max_rows limit -- this makes the visualization clunky\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "ROI = alt.selection_interval()\n",
    "click = alt.selection_multi()\n",
    "\n",
    "scatter_base = alt.Chart(annotation_play).mark_circle().encode(\n",
    "    x='UMAP x',\n",
    "    y='UMAP y',\n",
    "    color = alt.Color('Cluster:N', scale=alt.Scale(scheme = 'set1'))\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ").add_selection(ROI, click)\n",
    "\n",
    "br_sz = 20\n",
    "hist_base = alt.Chart(annotation_play).mark_bar(size=br_sz).encode(\n",
    "    x='ontology',\n",
    "    y='mean(value):Q'\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ")\n",
    "\n",
    "hist_roi = alt.Chart(annotation_play).mark_bar(size=br_sz, opacity=0.5).encode(\n",
    "    alt.X('ontology'),\n",
    "    alt.Y('mean(value):Q',\n",
    "          scale=alt.Scale(domain=(0,1))),\n",
    "    color=alt.value('red')\n",
    ").properties(\n",
    "    width = 500,\n",
    "    height = 200\n",
    ").transform_filter(\n",
    "    ROI | click\n",
    ")\n",
    "\n",
    "# hist_labels = alt.Chart(df_labels).mark_text(align='center', baseline='bottom',\n",
    "#                                  dy=35, fontSize=12\n",
    "#                                  ).encode(\n",
    "# y=alt.value(100),\n",
    "# text='labels:N')\n",
    "\n",
    "\n",
    "scatter_base & (hist_base + hist_roi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
